# 简介
[b站学习链接](https://www.bilibili.com/video/BV1JE411g7XF?from=search&seid=17781408679061316428)
[官方链接](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html)
机器学习就是利用机器自动找函数。举例来说，语音识别就是以"声音"作为输入，以"文字"作为输出的函数，只不过这个函数太复杂了，没有人可以写出它的函数表达式。

机器学习常用来生成几种函数：regression（回归）、classification（分类）、generation（生成）
机器如何可以推断出函数？
一种方式：
supervised learning（监督学习）：给机器一大堆带标签的数据（labeled data）
loss：机器通过损失函数判断自己找到的函数的好坏。
接下来机器自动找出loss最低的函数。

另一种方式：
reinforcement learning（强化学习）：像围棋AI，我们没法给自己带标签的数据集，告诉它现在的棋面下哪里最好，这需要太专业的人来做，我们让它自己跟自己下，或者跟别人下，在下的过程中下赢了，它通过这样的reward知道怎样下可以赢。早棋的 alpha go 就是先通过 supervised learning 得到基础的方法，再通过 reinforcement learning 增强方法；现在的 alpha go 完全通过 reinforcement learning 可以打遍天下无敌手。

还有一种方式：
unsupervised learning（无监督学习）：如果只给机器数据而没有标签，机器可以从这些数据中学到什么？

给机器一个范围，让机器在这个范围中找到最好的函数
机器通过gradient descent（梯度下降）的方法寻找函数

# regression
通过一个有趣的例子来学习regresion——预测一只宝可梦成长后的cp值。
新抓到的宝可梦有一些属性：种类、cp、hp、重量，通过这些属性预测成长后的cp值。
第一步：选择模型，我们选择最简单的模型，linear model（线性模型)，假设输出与输入是线性关系。
第二步：准备数据，培养很多只宝可梦，收集他们初始的各项数值与最终的cp值。
第三步：选择损失函数，使用均方误差。
第四步：从候选的模型中选择最佳的模型，在我们的例子中就是要确定最佳的参数。穷举可以解决我们的问题，但穷举漫无目标，花销太大。数学告诉我们gradient descent（梯度下降）可以用来进行这个过程，迭代出最佳的模型参数。dradient descent适用于处理所有可微分的函数。

gradient descent过程：
1. 选取初始的参数w0
2. 计算损失函数对w在w0处的微分值，如果微分值为负，说明在w0的邻域，随着w的增大，损失函数的结果在减小，而我们需要的就是损失函数最小时的w。此时，我们需要在w0的基础上增大w。应该增加多大，我们选取步长为`-a*dw`负的常数倍的微分值。常数的选取很关键。经过这样的分析我们就可以知道，梯度下降法只能找到一个极小值。在线性回归问题中，只有一个极小值就是最小值，所以我们不用担心。

如果预测的结果不尽如人意，可能是我们的model选取的不好，我们可能需要选取更好的model，例如二次式。在测试中我们发现，越复杂的model对训练集的匹配越好（曲线可以更加弯弯绕绕），但在测试集的偏差却变大，这种现象成为过拟合（overfitting）。越复杂的模型数据依赖性越高。 
